# Install all required packages.
install.packages(c("ggplot2", "e1071", "caret", "quanteda", 
                   "irlba", "randomForest"))

# Load up the .CSV data and explore in RStudio.
data.raw <- read.csv(file.choose(), stringsAsFactors = FALSE)
View(data.raw)

# Check data to see if there are missing values.
length(which(!complete.cases(data.raw)))

library(quanteda)
help(package = "quanteda")


# Tokenize the posts.
data.tokens <- tokens(data.raw$Text, what = "word", 
                      remove_numbers = TRUE, remove_punct = TRUE,
                      remove_symbols = TRUE, remove_hyphens = TRUE, remove_twitter = T, remove_url = T)


# Lower case the tokens.
data.tokens <- tokens_tolower(data.tokens)


# Use quanteda's built-in stopword list for English.
# NOTE - You should always inspect stopword lists for applicability to
#        your problem/domain.
data.tokens <- tokens_select(data.tokens, stopwords(), 
                             selection = "remove")


# Perform stemming on the tokens.
data.tokens <- tokens_wordstem(data.tokens, language = "english")

# Create our first bag-of-words model.
data.tokens.dfm <- dfm(data.tokens, tolower = FALSE)


# Transform to a matrix and inspect.
data.tokens.matrix <- as.matrix(data.tokens.dfm)
View(data.tokens.matrix[1:20, 1:100])
dim(data.tokens.matrix)


# Investigate the effects of stemming.
colnames(data.tokens.matrix)[1:50]

# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}

# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))
  
  log10(corpus.size / doc.count)
}

# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}


# First step, normalize all documents via TF.
data.tokens.df <- apply(data.tokens.matrix, 1, term.frequency)
dim(data.tokens.df)
View(data.tokens.df[1:20, 1:100])


# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
data.tokens.idf <- apply(data.tokens.matrix, 2, inverse.doc.freq)
str(data.tokens.idf)


# Lastly, calculate TF-IDF for our training corpus.
data.tokens.tfidf <-  apply(data.tokens.df, 2, tf.idf, idf = data.tokens.idf)
dim(data.tokens.tfidf)
View(data.tokens.tfidf[1:25, 1:25])

################################################################
#LDA function is not accepting the TF-IDF matrix because it contains non-integers. Have to figure out a work around for the same. 
#Meantime, LDA can be run on the term-frequency matrix. It works!
##################################################################

# Transpose the matrix
data.tokens.tfidf <- t(data.tokens.tfidf)
dim(data.tokens.tfidf)
View(data.tokens.tfidf[1:25, 1:25])

#Load Topic models
install.packages("topicmodels")
library(topicmodels)
#Run Latent Dirichlet Allocation (LDA) using Gibbs Sampling
#set burn in
burnin <-1000
#set iterations
iter<-2000
#thin the spaces between samples
thin <- 500
#set random starts at 5
nstart <-5
#use random integers as seed 
seed <- list(254672,109,122887,145629037,2)
# return the highest probability as the result
best <-TRUE
#set number of topics 
k <-5
#run the LDA model
data.lda <- LDA(data.tokens.tfidf,k, method="Gibbs", 
                control= list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))

top.words <- top.topic.words(ldaModel$topics, 5, by.score=TRUE)

terms(data.lda, 15)
################################################
control_LDA_Gibbs <- list(alpha = 50/k, estimate.beta = T, verbose = 0, prefix = tempfile(), save = 0, 
keep = 50, seed = 980,  for reproducibility nstart = 1, best = T,delta = 0.1,iter = 2000,burnin = 100,thin = 2000) 

data.lda2 <- LDA(data.tokens.tfidf, k, method = "Gibbs", control = control_LDA_Gibbs)

many_models <- mclapply(seq(2, 35, by = 1), function(x) {LDA(Blog_DTM, x, method = "Gibbs", control = control_LDA_Gibbs)} )
